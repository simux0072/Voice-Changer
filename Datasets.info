CREMA-D: Crowd-sourced Emotional Multimodal Actors:
///
CREMA-D is a dataset of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from various races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High and Unspecified). Participants rated the emotion and emotion levels based on the combined audiovisual presentation, the video alone, and the audio alone. Due to the large number of ratings needed, this effort was crowd-sourced, and a total of 2443 participants each rated 90 unique clips, 30 audio, 30 visual, and 30 audio-visual.
///
https://dagshub.com/mert.bozkirr/CREMA-D

Device and Produced Speech:
///
The DAPS (Device and Produced Speech) dataset is a collection of aligned versions of professionally produced studio speech recordings and recordings of the same speech on common consumer devices (tablet and smartphone) in real-world environments. It has 15 versions of audio (3 professional versions and 12 consumer device/real-world environment combinations). Each version consists of about 4 1/2 hours of data (about 14 minutes from each of 20 speakers).
///
https://dagshub.com/kinkusuma/daps-dataset

Deeply Vocal Characterizer:
///
The latter is a human nonverbal vocal sound dataset consisting of 56.7 hours of short clips from 1419 speakers, crowdsourced by the general public in South Korea. Also, the dataset includes metadata such as age, sex, noise level, and quality of utterance. This repo holds only 723 utterances (ca. 1% of the whole corpus) and is free to use under CC BY-NC-ND 4.0. For accessing the complete dataset under a more restrictive license, please contact deeplyinc.
///
https://dagshub.com/L-theorist/Deeply_Nonverbal_Vocalization_Dataset

EMOVO Corpus:
///
EMOVO Corpus database built from the voices of 6 actors who played 14 sentences simulating six emotional states (disgust, fear, anger, joy, surprise, sadness) plus the neutral state. These emotions are well-known found in most of the literature related to emotional speech. The recordings were made with professional equipment in the Fondazione Ugo Bordoni laboratories.
///
https://dagshub.com/kingabzpro/EMOVO

Flickr 8k Audio Caption Corpus:
///
The Flickr 8k Audio Caption Corpus contains 40,000 spoken audio captions in .wav audio format, one for each caption included in the train, dev, and test splits in the original corpus. The audio is sampled at 16000 Hz with 16-bit depth and stored in Microsoft WAVE audio format.
///
https://dagshub.com/michizhou/Flickr-Audio-Caption-Corpus

LJ Speech:
///
a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts were published between 1884 and 1964 and are in the public domain. The audio was recorded in 2016–17 by the LibriVox project and is also in the public domain.
///
https://dagshub.com/kinkusuma/lj-speech-dataset

Speech Commands Dataset:
///
The dataset (1.4 GB) has 65,000 one-second long utterances of 30 short words by thousands of different people, contributed by public members through the AIY website. This is a set of one-second .wav audio files, each containing a single spoken English word.
///
https://dagshub.com/kingabzpro/Speech_Commands_Dataset

TESS: Toronto Emotional Speech Set:
///
The Northwestern University Auditory Test №6 was used to create these stimuli. Two actresses (aged 26 and 64 years) recited a set of 200 target words in the carrier phrase “Say the word _____,” and recordings were produced of the set depicting each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are a total of 2800 stimuli.
///
https://dagshub.com/hazalkl/Toronto-emotional-speech-set-TESS

Voice Gender:
///
The VoxCeleb dataset (7000+ unique speakers and utterances, 3683 males / 2312 females). The VoxCeleb is an audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube. VoxCeleb contains speech from speakers spanning a wide range of different ethnicities, accents, professions, and ages.
///
https://dagshub.com/kingabzpro/voice_gender_detection

LibriSpeech:
///
This dataset is a large-scale corpus of around 1000 hours of English speech. The data has been sourced from audio books from the LibriVox project and is 60 GB in size.
///
http://www.openslr.org/12/

The Spoken Wikipedia Corpora:
///
This is a corpus of aligned spoken Wikipedia articles from the English, German, and Dutch Wikipedia. Hundreds of hours of aligned audio and annotations can be mapped back to the original HTML. The entire set is about 38 GB in size available in both audio and without audio format.
///
https://nats.gitlab.io/swc/

TED-LIUM:
///
Audio transcription of TED talks. 1495 TED talks audio recordings along with full-text transcriptions of those recordings, created by Laboratoire d’Informatique de l’Université du Maine (LIUM).
///
http://www.openslr.org/51/

Common Voice:
///
Common Voice (12 GB is size) is a corpus of speech data read by users on the Common Voice website, and based on text from a number of public domain sources like user-submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems.
///
https://www.kaggle.com/mozillaorg/common-voice/home

2000 HUB5 English:
///
English-only speech data used most recently in the Deep Speech paper from Baidu.
///
https://catalog.ldc.upenn.edu/LDC2002T43

TIMIT Corpus:
///
The TIMIT corpus (440 MB) of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. It includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16 kHz speech waveform file for each utterance.
///
https://github.com/philipperemy/timit/blob/master/README.md

Multimodal EmotionLines Dataset (MELD):
///
Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Each utterance in a dialogue has been labeled with— Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear.
///
https://github.com/SenticNet/MELD

